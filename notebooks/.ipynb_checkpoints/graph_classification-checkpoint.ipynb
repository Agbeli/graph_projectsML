{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification Project - MUTAG Dataset\n",
    "\n",
    "This notebook implements supervised machine learning on graph data using the MUTAG dataset.\n",
    "\n",
    "## Contents\n",
    "- Q1: Frequent Subgraph Mining + Classic ML\n",
    "- Q2: Graph Neural Networks (GCN, GIN, GraphSAGE, GAT)\n",
    "- Q3: Comparison of approaches\n",
    "- Q4: Explainability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install torch torch-geometric torch-scatter torch-sparse scikit-learn matplotlib seaborn networkx tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add source directory to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_mutag_dataset, pyg_to_networkx_graphs, split_dataset\n",
    "\n",
    "# Load MUTAG dataset\n",
    "dataset, info = load_mutag_dataset()\n",
    "\n",
    "# Display dataset info\n",
    "print(\"\\nDataset Information:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample graphs\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    data = dataset[idx]\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    colors = data.x.argmax(dim=1).numpy() if data.x is not None else 'skyblue'\n",
    "    \n",
    "    nx.draw(G, pos, ax=ax, with_labels=False, node_color=colors, \n",
    "            cmap=plt.cm.Set3, node_size=100, edge_color='gray')\n",
    "    \n",
    "    label = 'Mutagenic' if data.y.item() == 1 else 'Non-mutagenic'\n",
    "    ax.set_title(f'Graph {idx} ({label})\\nNodes: {data.num_nodes}, Edges: {data.num_edges}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/sample_graphs.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Frequent Subgraph Mining + Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_gspan_classic_ml import run_q1_experiments\n",
    "\n",
    "# Run Q1 experiments\n",
    "q1_results = run_q1_experiments(save_dir='../results/q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q1 results\n",
    "print(\"\\nSupport Threshold Ablation:\")\n",
    "display(q1_results['threshold_results'])\n",
    "\n",
    "# Plot threshold vs accuracy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(q1_results['threshold_results']['threshold'], \n",
    "         q1_results['threshold_results']['rf_accuracy'], 'o-', label='Accuracy')\n",
    "ax1.plot(q1_results['threshold_results']['threshold'], \n",
    "         q1_results['threshold_results']['rf_f1'], 's-', label='F1')\n",
    "ax1.set_xlabel('Support Threshold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Performance vs Support Threshold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(q1_results['threshold_results']['threshold'], \n",
    "         q1_results['threshold_results']['n_patterns'], 'o-', color='green')\n",
    "ax2.set_xlabel('Support Threshold')\n",
    "ax2.set_ylabel('Number of Patterns')\n",
    "ax2.set_title('Pattern Count vs Support Threshold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/q1/threshold_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "rf_importance = pd.read_csv('../results/q1/rf_feature_importance.csv')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = rf_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue', alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Important Subgraph Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/q1/feature_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q2_gnn_models import run_q2_experiments\n",
    "\n",
    "# Run Q2 experiments\n",
    "q2_results = run_q2_experiments(save_dir='../results/q2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GNN comparison\n",
    "print(\"\\nGNN Model Comparison:\")\n",
    "display(q2_results['model_results'])\n",
    "\n",
    "# Plot GNN comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1 = axes[0]\n",
    "models = q2_results['model_results']['model']\n",
    "accuracies = q2_results['model_results']['accuracy']\n",
    "ax1.bar(models, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('GNN Model Accuracy Comparison')\n",
    "ax1.set_ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Training time comparison\n",
    "ax2 = axes[1]\n",
    "train_times = q2_results['model_results']['train_time']\n",
    "ax2.bar(models, train_times, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "ax2.set_ylabel('Training Time (seconds)')\n",
    "ax2.set_title('GNN Training Time Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/q2/gnn_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers ablation visualization\n",
    "layers_df = q2_results['layers_ablation']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for model in layers_df['model'].unique():\n",
    "    model_data = layers_df[layers_df['model'] == model]\n",
    "    ax.plot(model_data['num_layers'], model_data['accuracy'], 'o-', label=model)\n",
    "\n",
    "ax.set_xlabel('Number of Layers')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Number of GNN Layers vs Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/q2/layers_ablation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q3_comparison import run_q3_experiments\n",
    "\n",
    "# Run Q3 experiments\n",
    "q3_results = run_q3_experiments(save_dir='../results/q3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison results\n",
    "print(\"\\nFull Comparison Results:\")\n",
    "display(q3_results['results'])\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(q3_results['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plot\n",
    "results_df = q3_results['results']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: All metrics heatmap\n",
    "ax1 = axes[0, 0]\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "heatmap_data = results_df.set_index('Model')[metrics]\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax1)\n",
    "ax1.set_title('Performance Metrics Heatmap')\n",
    "\n",
    "# Plot 2: Classic ML vs GNN Accuracy\n",
    "ax2 = axes[0, 1]\n",
    "classic = results_df[results_df['Approach'] == 'Classic ML']\n",
    "gnn = results_df[results_df['Approach'] == 'GNN']\n",
    "\n",
    "x = np.arange(max(len(classic), len(gnn)))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(range(len(classic)), classic['Accuracy'], width, label='Classic ML', color='steelblue')\n",
    "ax2.bar([i + len(classic) + 0.5 for i in range(len(gnn))], gnn['Accuracy'], width, label='GNN', color='coral')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy by Model')\n",
    "ax2.set_xticks(list(range(len(classic))) + [i + len(classic) + 0.5 for i in range(len(gnn))])\n",
    "ax2.set_xticklabels(list(classic['Model']) + list(gnn['Model']), rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Training time comparison\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['steelblue' if a == 'Classic ML' else 'coral' for a in results_df['Approach']]\n",
    "ax3.bar(results_df['Model'], results_df['Train Time'], color=colors)\n",
    "ax3.set_ylabel('Training Time (seconds)')\n",
    "ax3.set_title('Training Time by Model')\n",
    "ax3.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: F1 vs Training Time scatter\n",
    "ax4 = axes[1, 1]\n",
    "for approach in ['Classic ML', 'GNN']:\n",
    "    data = results_df[results_df['Approach'] == approach]\n",
    "    color = 'steelblue' if approach == 'Classic ML' else 'coral'\n",
    "    ax4.scatter(data['Train Time'], data['F1'], c=color, s=100, label=approach, alpha=0.7)\n",
    "    for i, row in data.iterrows():\n",
    "        ax4.annotate(row['Model'], (row['Train Time'], row['F1']), fontsize=8)\n",
    "\n",
    "ax4.set_xlabel('Training Time (seconds)')\n",
    "ax4.set_ylabel('F1 Score')\n",
    "ax4.set_title('F1 Score vs Training Time')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/q3/detailed_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q4_explainability import run_q4_experiments\n",
    "\n",
    "# Run Q4 experiments\n",
    "q4_results = run_q4_experiments(save_dir='../results/q4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display explainability results\n",
    "print(\"\\nGNNExplainer Evaluation Summary:\")\n",
    "display(q4_results['summary'])\n",
    "\n",
    "print(\"\\nTop Feature Importances (Classic ML):\")\n",
    "display(q4_results['feature_importance'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability visualization\n",
    "eval_results = q4_results['eval_results']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Fidelity+ distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(eval_results['fidelity_plus'], bins=15, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(eval_results['fidelity_plus'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {eval_results[\"fidelity_plus\"].mean():.3f}')\n",
    "ax1.set_xlabel('Fidelity+')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Fidelity+ Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# Fidelity- distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(eval_results['fidelity_minus'], bins=15, color='coral', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(eval_results['fidelity_minus'].mean(), color='red', linestyle='--',\n",
    "            label=f'Mean: {eval_results[\"fidelity_minus\"].mean():.3f}')\n",
    "ax2.set_xlabel('Fidelity-')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Fidelity- Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "# Sparsity distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(eval_results['sparsity'], bins=15, color='seagreen', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(eval_results['sparsity'].mean(), color='red', linestyle='--',\n",
    "            label=f'Mean: {eval_results[\"sparsity\"].mean():.3f}')\n",
    "ax3.set_xlabel('Sparsity')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Sparsity Distribution')\n",
    "ax3.legend()\n",
    "\n",
    "# Runtime distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(eval_results['runtime'], bins=15, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(eval_results['runtime'].mean(), color='red', linestyle='--',\n",
    "            label=f'Mean: {eval_results[\"runtime\"].mean():.3f}s')\n",
    "ax4.set_xlabel('Runtime (seconds)')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_title('Explanation Runtime Distribution')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/q4/explainability_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load all results\n",
    "q1_summary = pd.read_csv('../results/q1/q1_summary.csv')\n",
    "q2_summary = pd.read_csv('../results/q2/gnn_comparison.csv')\n",
    "q3_summary = pd.read_csv('../results/q3/comparison_results.csv')\n",
    "\n",
    "print(\"\\n1. BEST CLASSIC ML MODEL:\")\n",
    "best_classic = q1_summary.loc[q1_summary['Accuracy'].idxmax()]\n",
    "print(f\"   Model: {best_classic['Model']}\")\n",
    "print(f\"   Accuracy: {best_classic['Accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {best_classic['F1']:.4f}\")\n",
    "\n",
    "print(\"\\n2. BEST GNN MODEL:\")\n",
    "best_gnn = q2_summary.loc[q2_summary['accuracy'].idxmax()]\n",
    "print(f\"   Model: {best_gnn['model']}\")\n",
    "print(f\"   Accuracy: {best_gnn['accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {best_gnn['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n3. KEY FINDINGS:\")\n",
    "print(\"   - GNNs generally achieve slightly higher accuracy\")\n",
    "print(\"   - Classic ML provides faster inference\")\n",
    "print(\"   - Frequent subgraph patterns offer interpretable features\")\n",
    "print(\"   - GNNExplainer provides instance-level explanations\")\n",
    "\n",
    "print(\"\\n4. EXPLAINABILITY METRICS (GNNExplainer):\")\n",
    "q4_summary = pd.read_csv('../results/q4/explainability_summary.csv')\n",
    "for _, row in q4_summary.iterrows():\n",
    "    print(f\"   {row['Metric']}: {row['Mean']:.4f} Â± {row['Std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"\\nGenerated Result Files:\")\n",
    "for folder in ['q1', 'q2', 'q3', 'q4']:\n",
    "    path = f'../results/{folder}'\n",
    "    if os.path.exists(path):\n",
    "        files = os.listdir(path)\n",
    "        print(f\"\\n{folder.upper()}:\")\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
